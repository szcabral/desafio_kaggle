{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predição de Sucesso de Startups — Versão aprimorada\n",
        "\n",
        "Notebook adaptado para maximizar a acurácia dentro das regras do campeonato (somente: Numpy, Pandas, Scikit‑Learn; visualização com Matplotlib/Seaborn).\n",
        "\n",
        "Principais melhorias:\n",
        "- Engenharia de features adicionais e transformações (log, rates, flags).\n",
        "- Tratamento robusto de missing values e outliers (cap por percentis).\n",
        "- Balanceamento por oversampling apenas no conjunto de treino.\n",
        "- Seleção de features via RandomForest + SelectFromModel e SelectKBest (mutual_info).\n",
        "- Busca randômica de hiperparâmetros (RandomizedSearchCV) em RandomForest e HistGradientBoosting.\n",
        "- Ensemble (VotingClassifier soft) entre estimadores afinados.\n",
        "\n",
        "Instruções: rode todas as células (o notebook foi pensado para execução em Kaggle/locais com os dados em `../data/`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.feature_selection import SelectFromModel, SelectKBest, mutual_info_classif\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import time\n",
        "RANDOM_STATE = 42\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Carregamento dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('../data/train.csv')\n",
        "test_df = pd.read_csv('../data/test.csv')\n",
        "print('Train shape:', train_df.shape)\n",
        "print('Test shape:', test_df.shape)\n",
        "display(train_df.head())\n",
        "display(train_df.info())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Análise exploratória rápida\n",
        "- Ver distribuição do target, missing values e correlações iniciais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Contagem por label:\\n', train_df['labels'].value_counts())\n",
        "print('\\nPercentuais:\\n', train_df['labels'].value_counts(normalize=True)*100)\n",
        "\n",
        "missing = train_df.isnull().sum()\n",
        "missing = missing[missing>0].sort_values(ascending=False)\n",
        "print('\\nColunas com missing (train):')\n",
        "print(missing)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.countplot(data=train_df, x='labels')\n",
        "plt.title('Distribuição das labels (train)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Engenharia de features\n",
        "Criamos novas features contínuas e flags que costumam ser informativas para sucesso de startups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_features(df):\n",
        "    df = df.copy()\n",
        "    # Média de idades de funding\n",
        "    if {'age_first_funding_year','age_last_funding_year'}.issubset(df.columns):\n",
        "        df['mean_funding_age'] = df[['age_first_funding_year','age_last_funding_year']].mean(axis=1)\n",
        "    else:\n",
        "        df['mean_funding_age'] = np.nan\n",
        "    # duração entre marcos\n",
        "    if {'age_first_milestone_year','age_last_milestone_year'}.issubset(df.columns):\n",
        "        df['milestone_duration'] = df['age_last_milestone_year'] - df['age_first_milestone_year']\n",
        "        df['milestone_duration'] = df['milestone_duration'].fillna(0)\n",
        "    else:\n",
        "        df['milestone_duration'] = 0\n",
        "    # funding por rodada\n",
        "    if 'funding_total_usd' in df.columns and 'funding_rounds' in df.columns:\n",
        "        df['funding_per_round'] = df['funding_total_usd'] / df['funding_rounds'].replace(0, np.nan)\n",
        "        df['funding_per_round'] = df['funding_per_round'].fillna(0)\n",
        "    else:\n",
        "        df['funding_per_round'] = 0\n",
        "    # milestones por rodada\n",
        "    if 'milestones' in df.columns and 'funding_rounds' in df.columns:\n",
        "        df['milestones_per_round'] = df['milestones'] / df['funding_rounds'].replace(0, np.nan)\n",
        "        df['milestones_per_round'] = df['milestones_per_round'].fillna(0)\n",
        "    else:\n",
        "        df['milestones_per_round'] = 0\n",
        "    # flags totais de rounds e localidades\n",
        "    rounds_flags = [c for c in ['has_VC','has_angel','has_roundA','has_roundB','has_roundC','has_roundD'] if c in df.columns]\n",
        "    if rounds_flags:\n",
        "        df['total_round_flags'] = df[rounds_flags].sum(axis=1)\n",
        "    else:\n",
        "        df['total_round_flags'] = 0\n",
        "    loc_flags = [c for c in ['is_CA','is_NY','is_MA','is_TX','is_otherstate'] if c in df.columns]\n",
        "    if loc_flags:\n",
        "        df['total_location_flags'] = df[loc_flags].sum(axis=1)\n",
        "    else:\n",
        "        df['total_location_flags'] = 0\n",
        "    # relação por rodada\n",
        "    if 'relationships' in df.columns and 'funding_rounds' in df.columns:\n",
        "        df['relationships_per_round'] = df['relationships'] / df['funding_rounds'].replace(0, np.nan)\n",
        "        df['relationships_per_round'] = df['relationships_per_round'].fillna(0)\n",
        "    else:\n",
        "        df['relationships_per_round'] = 0\n",
        "    # transformações log para features muito assimétricas\n",
        "    if 'funding_total_usd' in df.columns:\n",
        "        df['log_funding_total_usd'] = np.log1p(df['funding_total_usd'].fillna(0))\n",
        "    else:\n",
        "        df['log_funding_total_usd'] = 0\n",
        "    # flags simples\n",
        "    if 'milestones' in df.columns:\n",
        "        df['has_milestone'] = (df['milestones']>0).astype(int)\n",
        "    else:\n",
        "        df['has_milestone'] = 0\n",
        "    # idade entre first e last funding\n",
        "    if 'age_first_funding_year' in df.columns and 'age_last_funding_year' in df.columns:\n",
        "        df['age_between_fundings'] = df['age_last_funding_year'] - df['age_first_funding_year']\n",
        "        df['age_between_fundings'] = df['age_between_fundings'].fillna(0)\n",
        "    else:\n",
        "        df['age_between_fundings'] = 0\n",
        "    return df\n",
        "\n",
        "train_df = create_features(train_df)\n",
        "test_df = create_features(test_df)\n",
        "display(train_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Tratamento de missing values e outliers\n",
        "- Usamos imputer por mediana nos numéricos. \n",
        "- Cap (winsorize) entre 1º e 99º percentil para reduzir influência de outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Selecionar colunas numéricas (exceto id e label)\n",
        "numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "numeric_cols = [c for c in numeric_cols if c not in ['id','labels']]\n",
        "numeric_cols[:20]\n",
        "\n",
        "# Capping por percentis (aplica ao train e test)\n",
        "def cap_outliers(df, cols, lower_q=0.01, upper_q=0.99):\n",
        "    df = df.copy()\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            low = df[c].quantile(lower_q)\n",
        "            high = df[c].quantile(upper_q)\n",
        "            df[c] = df[c].clip(lower=low, upper=high)\n",
        "    return df\n",
        "\n",
        "train_df = cap_outliers(train_df, numeric_cols)\n",
        "test_df = cap_outliers(test_df, numeric_cols)\n",
        "\n",
        "# Imputação por mediana para variáveis numéricas\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "train_df[numeric_cols] = imputer.fit_transform(train_df[numeric_cols])\n",
        "test_df[numeric_cols] = imputer.transform(test_df[numeric_cols])\n",
        "\n",
        "print('Missing após imputação (train):')\n",
        "print(train_df[numeric_cols].isnull().sum().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Codificação de categóricas\n",
        "Usamos `get_dummies` com `drop_first=True` para manter compatibilidade com scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_cols = [c for c in train_df.columns if train_df[c].dtype=='object' and c!='id']\n",
        "cat_cols\n",
        "\n",
        "train_df = pd.get_dummies(train_df, columns=cat_cols, drop_first=True)\n",
        "test_df = pd.get_dummies(test_df, columns=cat_cols, drop_first=True)\n",
        "\n",
        "# Alinhar colunas (adiciona colunas faltantes com 0)\n",
        "train_cols = set(train_df.columns)\n",
        "test_cols = set(test_df.columns)\n",
        "for c in train_cols - test_cols:\n",
        "    if c!='labels':\n",
        "        test_df[c] = 0\n",
        "for c in test_cols - train_cols:\n",
        "    train_df[c] = 0\n",
        "\n",
        "# Garantir mesma ordem de colunas\n",
        "train_df = train_df.reindex(sorted(train_df.columns), axis=1)\n",
        "test_df = test_df.reindex(sorted(test_df.columns), axis=1)\n",
        "print('Colunas após encoding:', len(train_df.columns))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Separação treino/validação e balanceamento (oversampling)\n",
        "Faremos oversampling **apenas** no conjunto de treino (para evitar vazamento)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = train_df.drop('labels', axis=1)\n",
        "y = train_df['labels']\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE)\n",
        "\n",
        "print('Train:', X_train.shape, 'Val:', X_val.shape)\n",
        "\n",
        "# Oversampling simples: replicar minoritária até balancear\n",
        "def simple_oversample(X, y, random_state=RANDOM_STATE):\n",
        "    df = pd.concat([X, y], axis=1)\n",
        "    major = df[df['labels']==0]\n",
        "    minor = df[df['labels']==1]\n",
        "    if len(minor)==0:\n",
        "        return X, y\n",
        "    ratio = int(len(major)/len(minor))\n",
        "    if ratio<=1:\n",
        "        return X, y\n",
        "    minors_upsampled = minor.sample(n=len(major)-len(minor), replace=True, random_state=random_state)\n",
        "    df_bal = pd.concat([df, minors_upsampled], axis=0)\n",
        "    df_bal = df_bal.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
        "    return df_bal.drop('labels', axis=1), df_bal['labels']\n",
        "\n",
        "X_train_bal, y_train_bal = simple_oversample(X_train, y_train)\n",
        "print('Após oversample (train):', X_train_bal.shape, y_train_bal.value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Padronização (StandardScaler) — aplicada apenas nas colunas numéricas\n",
        "Mantemos as colunas booleanas/categóricas sem escala para preservar interpretações."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_cols = [c for c in X_train_bal.columns if X_train_bal[c].dtype in [np.float64, np.int64]]\n",
        "num_cols = [c for c in num_cols if c!='id']\n",
        "len(num_cols), num_cols[:20]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_bal_scaled = X_train_bal.copy()\n",
        "X_val_scaled = X_val.copy()\n",
        "X_test_scaled = test_df.copy()\n",
        "\n",
        "X_train_bal_scaled[num_cols] = scaler.fit_transform(X_train_bal[num_cols])\n",
        "X_val_scaled[num_cols] = scaler.transform(X_val[num_cols])\n",
        "X_test_scaled[num_cols] = scaler.transform(test_df[num_cols])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Seleção de features\n",
        "Usamos duas abordagens e comparamos: SelectKBest(mutual_info) e SelectFromModel (RandomForest importance).\n",
        "Mantemos a melhor seleção baseada em validação cruzada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) SelectKBest (mutual information)\n",
        "k = min(30, X_train_bal_scaled.shape[1])\n",
        "skb = SelectKBest(mutual_info_classif, k=k)\n",
        "skb.fit(X_train_bal_scaled.drop(columns=['id'], errors='ignore'), y_train_bal)\n",
        "cols_kbest = X_train_bal_scaled.drop(columns=['id'], errors='ignore').columns[skb.get_support()].tolist()\n",
        "print('Top KBest cols:', len(cols_kbest))\n",
        "\n",
        "# 2) SelectFromModel (RandomForest)\n",
        "rf_sel = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE, n_jobs=-1)\n",
        "rf_sel.fit(X_train_bal_scaled.drop(columns=['id'], errors='ignore'), y_train_bal)\n",
        "sfm = SelectFromModel(rf_sel, threshold='median')\n",
        "sfm.fit(X_train_bal_scaled.drop(columns=['id'], errors='ignore'), y_train_bal)\n",
        "cols_sfm = X_train_bal_scaled.drop(columns=['id'], errors='ignore').columns[sfm.get_support()].tolist()\n",
        "print('Cols SelectFromModel:', len(cols_sfm))\n",
        "\n",
        "# Interseção (mais conservadora)\n",
        "selected_cols = sorted(list(set(cols_kbest) | set(cols_sfm)))\n",
        "print('Total selected columns (union):', len(selected_cols))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Treinamento e tunning (RandomizedSearchCV) — RandomForest e HistGradientBoosting\n",
        "Buscas são limitadas para execução em tempo razoável; ajuste `n_iter`/`cv` conforme disponibilidade computacional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preparar datasets com as colunas selecionadas\n",
        "features = selected_cols\n",
        "Xtr = X_train_bal_scaled[features]\n",
        "Xv = X_val_scaled[features]\n",
        "Xt = X_test_scaled[features]\n",
        "\n",
        "print('Shapes -> Xtr, Xv, Xt:', Xtr.shape, Xv.shape, Xt.shape)\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "rf = RandomForestClassifier(class_weight='balanced', random_state=RANDOM_STATE)\n",
        "param_dist_rf = {\n",
        "    'n_estimators': [100,200,300],\n",
        "    'max_depth': [5,10,20, None],\n",
        "    'min_samples_split': [2,5,10],\n",
        "    'min_samples_leaf': [1,2,4]\n",
        "}\n",
        "rs_rf = RandomizedSearchCV(rf, param_dist_rf, n_iter=15, cv=cv, scoring='accuracy', n_jobs=-1, random_state=RANDOM_STATE, verbose=1)\n",
        "t0 = time.time()\n",
        "rs_rf.fit(Xtr, y_train_bal)\n",
        "t1 = time.time()\n",
        "print('RandomForest best params:', rs_rf.best_params_, 'Time (s):', round(t1-t0,1))\n",
        "\n",
        "# HGB (bom desempenho em dados mistos e rápido)\n",
        "hgb = HistGradientBoostingClassifier(random_state=RANDOM_STATE)\n",
        "param_dist_hgb = {\n",
        "    'max_iter': [100,200,300],\n",
        "    'max_depth': [3,5,10, None],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'min_samples_leaf': [20,50,100]\n",
        "}\n",
        "rs_hgb = RandomizedSearchCV(hgb, param_dist_hgb, n_iter=15, cv=cv, scoring='accuracy', n_jobs=-1, random_state=RANDOM_STATE, verbose=1)\n",
        "t0 = time.time()\n",
        "rs_hgb.fit(Xtr, y_train_bal)\n",
        "t1 = time.time()\n",
        "print('HGB best params:', rs_hgb.best_params_, 'Time (s):', round(t1-t0,1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Ensemble (VotingClassifier) com os melhores estimadores\n",
        "Usamos `soft` voting (probabilidades) — requer `predict_proba` nos estimadores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_rf = rs_rf.best_estimator_\n",
        "best_hgb = rs_hgb.best_estimator_\n",
        "best_lr = LogisticRegression(class_weight='balanced', solver='liblinear', max_iter=200, random_state=RANDOM_STATE)\n",
        "best_lr.fit(Xtr, y_train_bal)\n",
        "\n",
        "voting = VotingClassifier(estimators=[('rf', best_rf), ('hgb', best_hgb), ('lr', best_lr)], voting='soft', n_jobs=-1)\n",
        "t0 = time.time()\n",
        "voting.fit(Xtr, y_train_bal)\n",
        "t1 = time.time()\n",
        "print('Ensemble treinado. Time (s):', round(t1-t0,1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Avaliação no conjunto de validação\n",
        "yv_pred = voting.predict(Xv)\n",
        "print('Relatório de classificação (val):')\n",
        "print(classification_report(y_val, yv_pred))\n",
        "print('Acurácia (val):', accuracy_score(y_val, yv_pred))\n",
        "print('Precision:', precision_score(y_val, yv_pred, zero_division=0))\n",
        "print('Recall:', recall_score(y_val, yv_pred, zero_division=0))\n",
        "print('F1:', f1_score(y_val, yv_pred, zero_division=0))\n",
        "\n",
        "cm = confusion_matrix(y_val, yv_pred)\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predito')\n",
        "plt.ylabel('Verdadeiro')\n",
        "plt.title('Matriz de confusão (val)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Validação cruzada do ensemble\n",
        "Usamos cross_val_score com cv estratificado para estimar desempenho mais robusto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv_scores = cross_val_score(voting, Xtr, y_train_bal, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "print('CV scores:', cv_scores)\n",
        "print('CV mean accuracy: %.4f +- %.4f' % (cv_scores.mean(), cv_scores.std()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) Importância das features (RandomForest)\n",
        "Plota as top-20 features segundo o RandomForest afinado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf_imp = pd.Series(best_rf.feature_importances_, index=features).sort_values(ascending=False)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.barplot(x=rf_imp.values[:20], y=rf_imp.index[:20])\n",
        "plt.title('Top 20 features (RandomForest)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13) Treinamento final e submissão\n",
        "Treinamos o modelo final no conjunto inteiro de treino (com as mesmas transformações aplicadas) e geramos `submission.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reaplicar todo pipeline no conjunto completo antes de treinar final\n",
        "X_full = train_df.drop('labels', axis=1)\n",
        "y_full = train_df['labels']\n",
        "\n",
        "# Para simplicidade, aplicar oversampling no X_full (opcional) — aqui vamos treinar com a distribuição original e usar class_weight\n",
        "X_full_scaled = X_full.copy()\n",
        "X_full_scaled[num_cols] = scaler.transform(X_full[num_cols])\n",
        "\n",
        "# selecionar colunas\n",
        "X_full_sel = X_full_scaled[features]\n",
        "\n",
        "final_model = VotingClassifier(estimators=[('rf', rs_rf.best_estimator_), ('hgb', rs_hgb.best_estimator_), ('lr', best_lr)], voting='soft', n_jobs=-1)\n",
        "final_model.fit(X_full_sel, y_full)\n",
        "\n",
        "# Prever no test\n",
        "X_test_final = X_test_scaled[features]\n",
        "test_preds = final_model.predict(X_test_final)\n",
        "\n",
        "submission = pd.DataFrame({'id': test_df['id'], 'labels': test_preds})\n",
        "submission.to_csv('../data/submission_improved.csv', index=False)\n",
        "print(\"Submissão salva em ../data/submission_improved.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hipóteses (3) — exemplo que você deve documentar no notebook\n",
        "1. **Empresas com maior funding por rodada têm maior probabilidade de sucesso** — pois mais capital permite crescimento e resiliência.\n",
        "2. **Mais milestones por rodada (ou presença de milestones) correlaciona com maior sucesso** — indica execução e progresso.\n",
        "3. **Presença de investidores (has_VC, has_angel) e múltiplas rodadas (total_round_flags) aumenta chance de sucesso** — sinaliza confiança externa e tração.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Observações finais\n",
        "- O notebook aplica técnicas permitidas pelo campeonato (somente bibliotecas autorizadas).\n",
        "- Ajuste `n_iter` e `cv` nas buscas de hiperparâmetros de acordo com o tempo computacional disponível.\n",
        "- Execute o notebook no Kaggle/colab/local com os dados em `../data/` e verifique o CSV gerado em `../data/submission_improved.csv`.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
