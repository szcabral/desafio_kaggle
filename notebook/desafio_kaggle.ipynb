{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2a7f34c",
   "metadata": {},
   "source": [
    "# Análise e Previsão de Sucesso de Startups\n",
    "\n",
    "Este notebook apresenta uma análise completa para prever o sucesso ou fracasso de startups com base em seus dados históricos, incluindo informações sobre investimentos, localização e características operacionais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1137c632",
   "metadata": {},
   "source": [
    "## 1. Configuração do Ambiente\n",
    "\n",
    "Nesta seção, importamos todas as bibliotecas necessárias e configuramos o ambiente de trabalho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f5c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação das bibliotecas necessárias\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Bibliotecas para análise de dados e visualização\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Bibliotecas para machine learning\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Configuração da semente aleatória para reprodutibilidade\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Configurações de visualização\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette('viridis')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6613cf3",
   "metadata": {},
   "source": [
    "## 2. Carregamento e Exploração Inicial dos Dados\n",
    "\n",
    "Nesta seção, vamos carregar os dados e fazer uma exploração inicial para entender sua estrutura e características principais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7882d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento dos datasets de treino e teste\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')\n",
    "\n",
    "# Exibindo informações básicas sobre os datasets\n",
    "print(\"=== Dataset de Treino ===\")\n",
    "print(f\"Dimensões: {train_df.shape}\")\n",
    "print(\"\\nPrimeiras linhas:\")\n",
    "display(train_df.head())\n",
    "print(\"\\nInformações sobre as colunas:\")\n",
    "print(train_df.info())\n",
    "\n",
    "print(\"\\n=== Dataset de Teste ===\")\n",
    "print(f\"Dimensões: {test_df.shape}\")\n",
    "print(\"\\nPrimeiras linhas:\")\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590ae304",
   "metadata": {},
   "source": [
    "### 2.1 Análise da Variável Alvo\n",
    "\n",
    "Vamos analisar a distribuição da nossa variável alvo (sucesso/insucesso) para entender o balanceamento das classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084f68b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise da distribuição da variável alvo\n",
    "labels_dist = train_df['labels'].value_counts()\n",
    "labels_pct = train_df['labels'].value_counts(normalize=True)\n",
    "\n",
    "# Criando um gráfico de barras para visualizar a distribuição\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=train_df, x='labels')\n",
    "plt.title('Distribuição da Variável Alvo')\n",
    "plt.xlabel('Rótulo (0=Insucesso, 1=Sucesso)')\n",
    "plt.ylabel('Quantidade')\n",
    "\n",
    "# Adicionando as porcentagens nas barras\n",
    "total = len(train_df['labels'])\n",
    "for i, v in enumerate(labels_dist):\n",
    "    plt.text(i, v, f'{labels_pct[i]:.1%}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDistribuição detalhada:\")\n",
    "print(f\"Sucesso (1): {labels_dist[1]} casos ({labels_pct[1]:.1%})\")\n",
    "print(f\"Insucesso (0): {labels_dist[0]} casos ({labels_pct[0]:.1%})\")\n",
    "print(f\"\\nRazão de desbalanceamento: {max(labels_pct)/min(labels_pct):.2f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d55abc",
   "metadata": {},
   "source": [
    "### 2.2 Análise de Valores Ausentes\n",
    "\n",
    "Vamos verificar se existem valores ausentes nos nossos dados e visualizar sua distribuição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa8a8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de valores ausentes nos datasets de treino e teste\n",
    "def analyze_missing_values(df, title):\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Valores Ausentes': missing,\n",
    "        'Porcentagem (%)': missing_pct\n",
    "    })\n",
    "    missing_df = missing_df[missing_df['Valores Ausentes'] > 0].sort_values('Valores Ausentes', ascending=False)\n",
    "    \n",
    "    if len(missing_df) > 0:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.barh(y=missing_df.index, width=missing_df['Porcentagem (%)'])\n",
    "        plt.title(f'Porcentagem de Valores Ausentes - {title}')\n",
    "        plt.xlabel('Porcentagem de Valores Ausentes')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nDetalhamento de valores ausentes - {title}:\")\n",
    "        print(missing_df)\n",
    "    else:\n",
    "        print(f\"\\n{title}: Não foram encontrados valores ausentes!\")\n",
    "\n",
    "# Análise para o dataset de treino\n",
    "analyze_missing_values(train_df, \"Dataset de Treino\")\n",
    "\n",
    "# Análise para o dataset de teste\n",
    "analyze_missing_values(test_df, \"Dataset de Teste\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5566dc6",
   "metadata": {},
   "source": [
    "### 2.3 Análise de Correlações\n",
    "\n",
    "Vamos analisar as correlações entre as variáveis numéricas e identificar padrões importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee527ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionando apenas as colunas numéricas\n",
    "numeric_cols = train_df.select_dtypes(include=[np.number]).columns.drop(['id', 'labels'], errors='ignore')\n",
    "\n",
    "# Calculando a matriz de correlação\n",
    "corr_matrix = train_df[numeric_cols].corr()\n",
    "\n",
    "# Criando o heatmap\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            fmt='.2f', square=True, linewidths=0.5)\n",
    "plt.title('Matriz de Correlação - Features Numéricas')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analisando correlações com a variável alvo\n",
    "target_corr = corr_matrix['labels'].sort_values(ascending=False)\n",
    "print(\"\\nTop 10 features mais correlacionadas com o sucesso:\")\n",
    "print(target_corr.head(10))\n",
    "print(\"\\nTop 10 features mais correlacionadas com o insucesso:\")\n",
    "print(target_corr.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629f73c5",
   "metadata": {},
   "source": [
    "## 3. Pré-processamento dos Dados\n",
    "\n",
    "Nesta seção, vamos realizar as seguintes etapas:\n",
    "1. Tratamento de valores ausentes\n",
    "2. Tratamento de outliers\n",
    "3. Engenharia de features\n",
    "4. Normalização dos dados\n",
    "5. Codificação de variáveis categóricas\n",
    "\n",
    "### 3.1 Tratamento de Valores Ausentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085bf3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma função para tratar valores ausentes\n",
    "def handle_missing_values(train_df, test_df):\n",
    "    # Criando cópias para não modificar os dataframes originais\n",
    "    train = train_df.copy()\n",
    "    test = test_df.copy()\n",
    "    \n",
    "    # Identificando colunas numéricas\n",
    "    numeric_cols = train.select_dtypes(include=[np.number]).columns.drop(['id', 'labels'], errors='ignore')\n",
    "    \n",
    "    # Usando SimpleImputer para preencher valores ausentes com a mediana\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    \n",
    "    # Aplicando o imputer nas colunas numéricas\n",
    "    train[numeric_cols] = imputer.fit_transform(train[numeric_cols])\n",
    "    test[numeric_cols] = imputer.transform(test[numeric_cols])\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# Aplicando o tratamento de valores ausentes\n",
    "train_clean, test_clean = handle_missing_values(train_df, test_df)\n",
    "\n",
    "print(\"Verificando valores ausentes após o tratamento:\")\n",
    "print(\"\\nDataset de Treino:\")\n",
    "print(train_clean.isnull().sum().sum(), \"valores ausentes\")\n",
    "print(\"\\nDataset de Teste:\")\n",
    "print(test_clean.isnull().sum().sum(), \"valores ausentes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abab0662",
   "metadata": {},
   "source": [
    "### 3.2 Tratamento de Outliers\n",
    "\n",
    "Vamos identificar e tratar outliers usando o método de capping nos percentis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe83ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para tratar outliers usando o método de capping\n",
    "def handle_outliers(train_df, test_df, lower_percentile=1, upper_percentile=99):\n",
    "    train = train_df.copy()\n",
    "    test = test_df.copy()\n",
    "    \n",
    "    numeric_cols = train.select_dtypes(include=[np.number]).columns.drop(['id', 'labels'], errors='ignore')\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        # Calculando os limites baseados nos percentis do conjunto de treino\n",
    "        lower_limit = np.percentile(train[col], lower_percentile)\n",
    "        upper_limit = np.percentile(train[col], upper_percentile)\n",
    "        \n",
    "        # Aplicando o capping em ambos os conjuntos\n",
    "        train[col] = train[col].clip(lower=lower_limit, upper=upper_limit)\n",
    "        test[col] = test[col].clip(lower=lower_limit, upper=upper_limit)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# Aplicando o tratamento de outliers\n",
    "train_clean, test_clean = handle_outliers(train_clean, test_clean)\n",
    "\n",
    "# Visualizando a distribuição antes e depois para algumas features importantes\n",
    "features_to_check = ['funding_total_usd', 'age_first_funding_year', 'relationships']\n",
    "\n",
    "fig, axes = plt.subplots(len(features_to_check), 2, figsize=(15, 5*len(features_to_check)))\n",
    "for idx, feature in enumerate(features_to_check):\n",
    "    if feature in train_df.columns:\n",
    "        # Antes do tratamento\n",
    "        sns.boxplot(data=train_df, y=feature, ax=axes[idx, 0])\n",
    "        axes[idx, 0].set_title(f'{feature} - Antes do Tratamento')\n",
    "        \n",
    "        # Depois do tratamento\n",
    "        sns.boxplot(data=train_clean, y=feature, ax=axes[idx, 1])\n",
    "        axes[idx, 1].set_title(f'{feature} - Depois do Tratamento')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b2ac22",
   "metadata": {},
   "source": [
    "### 3.3 Criando Features\n",
    "\n",
    "Vamos criar novas features baseadas nas existentes para capturar relações importantes nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d862d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criar novas features\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Features relacionadas a funding\n",
    "    df['funding_per_round'] = df['funding_total_usd'] / df['funding_rounds'].replace(0, np.nan)\n",
    "    df['log_funding_total'] = np.log1p(df['funding_total_usd'])\n",
    "    \n",
    "    # Features temporais\n",
    "    if 'age_first_funding_year' in df.columns and 'age_last_funding_year' in df.columns:\n",
    "        df['funding_duration'] = df['age_last_funding_year'] - df['age_first_funding_year']\n",
    "        df['funding_frequency'] = df['funding_rounds'] / df['funding_duration'].replace(0, np.nan)\n",
    "    \n",
    "    # Features de relacionamento\n",
    "    if 'relationships' in df.columns and 'funding_rounds' in df.columns:\n",
    "        df['relationships_per_round'] = df['relationships'] / df['funding_rounds'].replace(0, np.nan)\n",
    "    \n",
    "    # Features de milestone\n",
    "    if 'milestones' in df.columns:\n",
    "        df['has_milestones'] = (df['milestones'] > 0).astype(int)\n",
    "        if 'funding_rounds' in df.columns:\n",
    "            df['milestones_per_round'] = df['milestones'] / df['funding_rounds'].replace(0, np.nan)\n",
    "    \n",
    "    # Preenchendo NaN com 0\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Aplicando a engenharia de features\n",
    "train_processed = create_features(train_clean)\n",
    "test_processed = create_features(test_clean)\n",
    "\n",
    "# Exibindo as novas features criadas\n",
    "new_features = [col for col in train_processed.columns if col not in train_clean.columns]\n",
    "print(\"Novas features criadas:\")\n",
    "for feature in new_features:\n",
    "    print(f\"- {feature}\")\n",
    "\n",
    "# Visualizando a distribuição de algumas das novas features\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, feature in enumerate(new_features[:3]):  # Mostrando as 3 primeiras features\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    sns.histplot(data=train_processed, x=feature, hue='labels', bins=30, multiple=\"stack\")\n",
    "    plt.title(f'Distribuição de {feature}')\n",
    "    plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47b2f1b",
   "metadata": {},
   "source": [
    "### 3.4 Normalização dos Dados\n",
    "\n",
    "Vamos normalizar as features numéricas para garantir que todas estejam na mesma escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0867f5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificando features numéricas para normalização\n",
    "numeric_features = train_processed.select_dtypes(include=[np.number]).columns.drop(['id', 'labels'], errors='ignore')\n",
    "\n",
    "# Inicializando o StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Aplicando a normalização\n",
    "train_processed[numeric_features] = scaler.fit_transform(train_processed[numeric_features])\n",
    "test_processed[numeric_features] = scaler.transform(test_processed[numeric_features])\n",
    "\n",
    "# Verificando a distribuição após a normalização\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, feature in enumerate(numeric_features[:3]):  # Mostrando as 3 primeiras features\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    sns.histplot(data=train_processed, x=feature, bins=30)\n",
    "    plt.title(f'{feature}\\nMédia: {train_processed[feature].mean():.2f}\\nStd: {train_processed[feature].std():.2f}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVerificação das estatísticas após normalização:\")\n",
    "print(train_processed[numeric_features].describe().round(2).iloc[[1,2], :3])  # Mostrando média e desvio padrão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c724fe1e",
   "metadata": {},
   "source": [
    "## 4. Formulação e Análise de Hipóteses\n",
    "\n",
    "Antes de prosseguir com a modelagem, vamos formular e testar algumas hipóteses importantes sobre os fatores que podem influenciar o sucesso das startups.\n",
    "\n",
    "### Hipóteses Principais:\n",
    "\n",
    "1. **Hipótese do Volume de Financiamento (H1)**\n",
    "   - *Hipótese*: Startups que captam maior volume total de recursos têm maior probabilidade de sucesso\n",
    "   - *Justificativa*: Maior capital permite:\n",
    "     - Maior investimento em crescimento\n",
    "     - Mais recursos para desenvolvimento de produto\n",
    "     - Maior resistência a períodos de crise\n",
    "     - Capacidade de atrair talentos melhores\n",
    "\n",
    "2. **Hipótese da Maturidade do Funding (H2)**\n",
    "   - *Hipótese*: Startups que alcançam rodadas mais avançadas (B, C, D) têm maior taxa de sucesso\n",
    "   - *Justificativa*:\n",
    "     - Validação progressiva do modelo de negócio\n",
    "     - Due diligence mais rigorosa a cada rodada\n",
    "     - Demonstração de crescimento sustentável\n",
    "     - Maior experiência da equipe gestora\n",
    "\n",
    "3. **Hipótese do Network Effect (H3)**\n",
    "   - *Hipótese*: Startups com mais relacionamentos têm maior probabilidade de sucesso\n",
    "   - *Justificativa*:\n",
    "     - Acesso a mais recursos e oportunidades\n",
    "     - Maior rede de mentores e advisors\n",
    "     - Melhor capacidade de estabelecer parcerias\n",
    "     - Maior facilidade para novas rodadas\n",
    "\n",
    "Vamos analisar cada uma dessas hipóteses com visualizações e testes estatísticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108b8083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando o ambiente de visualização\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = (16, 12)\n",
    "\n",
    "# Criando a figura para as análises\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "\n",
    "# H1: Análise do Volume de Financiamento\n",
    "# Plot 1: Distribuição do financiamento total por status\n",
    "sns.boxplot(data=train_processed, x='labels', y='log_funding_total', ax=axes[0,0])\n",
    "axes[0,0].set_title('H1.a: Distribuição do Volume de Financiamento por Status')\n",
    "axes[0,0].set_xlabel('Status (0=Insucesso, 1=Sucesso)')\n",
    "axes[0,0].set_ylabel('Log do Financiamento Total')\n",
    "\n",
    "# Plot 2: Densidade do financiamento\n",
    "sns.kdeplot(data=train_processed, x='log_funding_total', hue='labels', ax=axes[0,1])\n",
    "axes[0,1].set_title('H1.b: Densidade do Volume de Financiamento por Status')\n",
    "axes[0,1].set_xlabel('Log do Financiamento Total')\n",
    "axes[0,1].set_ylabel('Densidade')\n",
    "\n",
    "# H2: Análise da Maturidade do Funding\n",
    "# Plot 3: Taxa de sucesso por número de rodadas avançadas\n",
    "round_cols = ['has_roundB', 'has_roundC', 'has_roundD']\n",
    "train_processed['advanced_rounds'] = train_processed[round_cols].sum(axis=1)\n",
    "sns.barplot(data=train_processed, x='advanced_rounds', y='labels', ax=axes[1,0])\n",
    "axes[1,0].set_title('H2.a: Taxa de Sucesso por Número de Rodadas Avançadas')\n",
    "axes[1,0].set_xlabel('Número de Rodadas Avançadas (B/C/D)')\n",
    "axes[1,0].set_ylabel('Taxa de Sucesso')\n",
    "\n",
    "# Plot 4: Distribuição de rodadas por status\n",
    "success_rounds = train_processed[train_processed['labels']==1]['advanced_rounds'].value_counts()\n",
    "failure_rounds = train_processed[train_processed['labels']==0]['advanced_rounds'].value_counts()\n",
    "combined_df = pd.DataFrame({'Sucesso': success_rounds, 'Insucesso': failure_rounds}).fillna(0)\n",
    "combined_df.plot(kind='bar', ax=axes[1,1])\n",
    "axes[1,1].set_title('H2.b: Distribuição do Número de Rodadas por Status')\n",
    "axes[1,1].set_xlabel('Número de Rodadas Avançadas')\n",
    "axes[1,1].set_ylabel('Quantidade de Startups')\n",
    "\n",
    "# H3: Análise do Network Effect\n",
    "# Plot 5: Relacionamentos por status\n",
    "sns.boxplot(data=train_processed, x='labels', y='relationships', ax=axes[2,0])\n",
    "axes[2,0].set_title('H3.a: Distribuição de Relacionamentos por Status')\n",
    "axes[2,0].set_xlabel('Status (0=Insucesso, 1=Sucesso)')\n",
    "axes[2,0].set_ylabel('Número de Relacionamentos')\n",
    "\n",
    "# Plot 6: Relacionamentos por rodada vs. Status\n",
    "sns.scatterplot(data=train_processed, x='relationships_per_round', y='relationships', \n",
    "                hue='labels', size='funding_total_usd', alpha=0.6, ax=axes[2,1])\n",
    "axes[2,1].set_title('H3.b: Relacionamentos vs. Relacionamentos por Rodada')\n",
    "axes[2,1].set_xlabel('Relacionamentos por Rodada')\n",
    "axes[2,1].set_ylabel('Número Total de Relacionamentos')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Análise estatística\n",
    "print(\"\\n=== Análise Estatística das Hipóteses ===\")\n",
    "\n",
    "# H1: Volume de Financiamento\n",
    "print(\"\\nH1 - Volume de Financiamento:\")\n",
    "h1_stats = train_processed.groupby('labels')['log_funding_total'].agg(['mean', 'median'])\n",
    "print(f\"Média de financiamento (log) para sucessos: {h1_stats.loc[1, 'mean']:.2f}\")\n",
    "print(f\"Média de financiamento (log) para insucessos: {h1_stats.loc[0, 'mean']:.2f}\")\n",
    "print(f\"Diferença relativa: {((h1_stats.loc[1, 'mean'] / h1_stats.loc[0, 'mean']) - 1) * 100:.1f}%\")\n",
    "\n",
    "# H2: Maturidade do Funding\n",
    "print(\"\\nH2 - Maturidade do Funding:\")\n",
    "h2_stats = train_processed.groupby('advanced_rounds')['labels'].agg(['mean', 'count'])\n",
    "print(\"Taxa de sucesso por número de rodadas avançadas:\")\n",
    "print(h2_stats['mean'].round(3))\n",
    "\n",
    "# H3: Network Effect\n",
    "print(\"\\nH3 - Network Effect:\")\n",
    "h3_stats = train_processed.groupby('labels')['relationships'].agg(['mean', 'median'])\n",
    "print(f\"Média de relacionamentos para sucessos: {h3_stats.loc[1, 'mean']:.2f}\")\n",
    "print(f\"Média de relacionamentos para insucessos: {h3_stats.loc[0, 'mean']:.2f}\")\n",
    "print(f\"Diferença relativa: {((h3_stats.loc[1, 'mean'] / h3_stats.loc[0, 'mean']) - 1) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e693d74",
   "metadata": {},
   "source": [
    "### Conclusões da Análise de Hipóteses\n",
    "\n",
    "#### H1: Volume de Financiamento\n",
    "**CONFIRMADA**\n",
    "- A análise mostra uma diferença significativa no volume de financiamento entre startups bem-sucedidas e malsucedidas\n",
    "- As startups bem-sucedidas têm, em média, um volume de financiamento consideravelmente maior\n",
    "- A distribuição dos valores mostra uma clara separação entre os grupos\n",
    "- **Implicação**: O volume de financiamento é um forte preditor de sucesso\n",
    "\n",
    "#### H2: Maturidade do Funding\n",
    "**CONFIRMADA**\n",
    "- Existe uma correlação positiva forte entre o número de rodadas avançadas e o sucesso\n",
    "- A taxa de sucesso aumenta consistentemente com cada rodada adicional\n",
    "- Startups que alcançam rodadas mais avançadas têm probabilidade significativamente maior de sucesso\n",
    "- **Implicação**: A progressão através das rodadas de financiamento é um indicador robusto de sucesso\n",
    "\n",
    "#### H3: Network Effect\n",
    "**CONFIRMADA**\n",
    "- Startups bem-sucedidas têm, em média, um número significativamente maior de relacionamentos\n",
    "- A qualidade dos relacionamentos (medida por relacionamentos por rodada) também é maior em casos de sucesso\n",
    "- Existe uma correlação positiva entre o tamanho da rede e o volume de financiamento\n",
    "- **Implicação**: O network é um fator crucial para o sucesso das startups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aec63c6",
   "metadata": {},
   "source": [
    "## 4. Modelagem\n",
    "\n",
    "### 4.1 Divisão dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868a9d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando features e target\n",
    "X = train_processed.drop(['id', 'labels'], axis=1)\n",
    "y = train_processed['labels']\n",
    "\n",
    "# Dividindo os dados em treino e validação\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Dimensões dos conjuntos de dados:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}\")\n",
    "print(f\"\\nDistribuição das classes:\")\n",
    "print(\"Treino:\")\n",
    "print(pd.Series(y_train).value_counts(normalize=True).round(3))\n",
    "print(\"\\nValidação:\")\n",
    "print(pd.Series(y_val).value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dc53c7",
   "metadata": {},
   "source": [
    "### 4.2 Treinamento dos Modelos\n",
    "\n",
    "Vamos treinar diferentes modelos e comparar seus desempenhos:\n",
    "1. Random Forest\n",
    "2. Gradient Boosting\n",
    "3. Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28c4c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo os modelos\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),\n",
    "    'Gradient Boosting': HistGradientBoostingClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "# Treinando e avaliando cada modelo\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Treinamento\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predições\n",
    "    train_pred = model.predict(X_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Métricas\n",
    "    results[name] = {\n",
    "        'Acurácia Treino': accuracy_score(y_train, train_pred),\n",
    "        'Acurácia Validação': accuracy_score(y_val, val_pred),\n",
    "        'Report Validação': classification_report(y_val, val_pred)\n",
    "    }\n",
    "\n",
    "# Exibindo resultados\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Acurácia no Treino: {metrics['Acurácia Treino']:.4f}\")\n",
    "    print(f\"Acurácia na Validação: {metrics['Acurácia Validação']:.4f}\")\n",
    "    print(\"\\nReport Detalhado (Validação):\")\n",
    "    print(metrics['Report Validação'])\n",
    "\n",
    "# Visualizando comparação de acurácias\n",
    "plt.figure(figsize=(10, 5))\n",
    "accuracies = pd.DataFrame({\n",
    "    'Modelo': list(results.keys()),\n",
    "    'Treino': [m['Acurácia Treino'] for m in results.values()],\n",
    "    'Validação': [m['Acurácia Validação'] for m in results.values()]\n",
    "})\n",
    "accuracies_melted = pd.melt(accuracies, id_vars=['Modelo'], var_name='Conjunto', value_name='Acurácia')\n",
    "sns.barplot(data=accuracies_melted, x='Modelo', y='Acurácia', hue='Conjunto')\n",
    "plt.title('Comparação de Acurácia entre Modelos')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37ed226",
   "metadata": {},
   "source": [
    "### 4.3 Otimização de Hiperparâmetros\n",
    "\n",
    "Vamos realizar uma busca pelos melhores hiperparâmetros para o modelo com melhor desempenho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb8996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo os espaços de hiperparâmetros para o RandomForest\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Inicializando RandomizedSearchCV\n",
    "rf_random = RandomizedSearchCV(\n",
    "    RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Realizando a busca\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nMelhores hiperparâmetros encontrados:\")\n",
    "print(rf_random.best_params_)\n",
    "print(f\"\\nMelhor acurácia na validação cruzada: {rf_random.best_score_:.4f}\")\n",
    "\n",
    "# Avaliando o modelo otimizado no conjunto de validação\n",
    "best_rf = rf_random.best_estimator_\n",
    "y_pred = best_rf.predict(X_val)\n",
    "print(\"\\nPerformance no conjunto de validação:\")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff00cf7",
   "metadata": {},
   "source": [
    "## 5. Geração das Predições Finais\n",
    "\n",
    "Agora vamos usar o modelo otimizado para fazer as predições no conjunto de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56acebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo predições no conjunto de teste\n",
    "X_test = test_processed.drop('id', axis=1)\n",
    "test_predictions = best_rf.predict(X_test)\n",
    "\n",
    "# Criando o arquivo de submissão\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_processed['id'],\n",
    "    'labels': test_predictions\n",
    "})\n",
    "\n",
    "# Salvando o arquivo de submissão\n",
    "submission.to_csv('../data/submission_improved_better.csv', index=False)\n",
    "\n",
    "# Exibindo a distribuição das predições\n",
    "print(\"Distribuição das predições no conjunto de teste:\")\n",
    "print(pd.Series(test_predictions).value_counts(normalize=True).round(3))\n",
    "\n",
    "# Visualizando a distribuição\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x=test_predictions)\n",
    "plt.title('Distribuição das Predições no Conjunto de Teste')\n",
    "plt.xlabel('Rótulo (0=Insucesso, 1=Sucesso)')\n",
    "plt.ylabel('Quantidade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bd46d4",
   "metadata": {},
   "source": [
    "## 6. Conclusões\n",
    "\n",
    "Neste notebook, realizamos uma análise completa com o objetivo de prever o sucesso de startups. O trabalho iniciou-se com a análise exploratória dos dados, na qual identificamos padrões relevantes, investigamos correlações entre variáveis, além de tratar valores ausentes e outliers para garantir maior consistência. Em seguida, aplicamos técnicas de engenharia de features, criando novas variáveis derivadas, normalizando os dados e selecionando as características mais representativas para o problema. Na etapa de modelagem, testamos diferentes algoritmos de aprendizado de máquina, realizamos a otimização de hiperparâmetros e avaliamos o desempenho por meio de validação, garantindo maior robustez. Por fim, os resultados apontaram que o modelo Random Forest otimizado apresentou a melhor performance, com boa acurácia em validação cruzada e predições equilibradas no conjunto de teste, mostrando-se adequado para a tarefa proposta."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
