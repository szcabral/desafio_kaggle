{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2a7f34c",
   "metadata": {},
   "source": [
    "# Análise e Previsão de Sucesso de Startups\n",
    "\n",
    "## Objetivo\n",
    "Este notebook apresenta uma análise completa para prever o sucesso ou fracasso de startups com base em seus dados históricos, incluindo informações sobre investimentos, localização e características operacionais.\n",
    "\n",
    "### Principais Objetivos:\n",
    "1. Realizar uma análise exploratória detalhada dos dados\n",
    "2. Identificar padrões e fatores que influenciam o sucesso das startups\n",
    "3. Desenvolver um modelo preditivo com alta acurácia\n",
    "4. Gerar insights acionáveis para stakeholders\n",
    "\n",
    "### Métrica de Avaliação\n",
    "A métrica principal será a **Acurácia** (percentual de previsões corretas) com meta mínima de 80%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1137c632",
   "metadata": {},
   "source": [
    "## 1. Configuração do Ambiente\n",
    "\n",
    "Nesta seção, importamos todas as bibliotecas necessárias e configuramos o ambiente de trabalho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f5c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação das bibliotecas necessárias\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Bibliotecas para análise de dados e visualização\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Bibliotecas para machine learning\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Configuração da semente aleatória para reprodutibilidade\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Configurações de visualização\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette('viridis')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6613cf3",
   "metadata": {},
   "source": [
    "## 2. Carregamento e Exploração Inicial dos Dados\n",
    "\n",
    "Nesta seção, vamos carregar os dados e fazer uma exploração inicial para entender sua estrutura e características principais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7882d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamento dos datasets de treino e teste\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')\n",
    "\n",
    "# Exibindo informações básicas sobre os datasets\n",
    "print(\"=== Dataset de Treino ===\")\n",
    "print(f\"Dimensões: {train_df.shape}\")\n",
    "print(\"\\nPrimeiras linhas:\")\n",
    "display(train_df.head())\n",
    "print(\"\\nInformações sobre as colunas:\")\n",
    "print(train_df.info())\n",
    "\n",
    "print(\"\\n=== Dataset de Teste ===\")\n",
    "print(f\"Dimensões: {test_df.shape}\")\n",
    "print(\"\\nPrimeiras linhas:\")\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590ae304",
   "metadata": {},
   "source": [
    "### 2.1 Análise da Variável Alvo\n",
    "\n",
    "Vamos analisar a distribuição da nossa variável alvo (sucesso/insucesso) para entender o balanceamento das classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084f68b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise da distribuição da variável alvo\n",
    "labels_dist = train_df['labels'].value_counts()\n",
    "labels_pct = train_df['labels'].value_counts(normalize=True)\n",
    "\n",
    "# Criando um gráfico de barras para visualizar a distribuição\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=train_df, x='labels')\n",
    "plt.title('Distribuição da Variável Alvo')\n",
    "plt.xlabel('Rótulo (0=Insucesso, 1=Sucesso)')\n",
    "plt.ylabel('Quantidade')\n",
    "\n",
    "# Adicionando as porcentagens nas barras\n",
    "total = len(train_df['labels'])\n",
    "for i, v in enumerate(labels_dist):\n",
    "    plt.text(i, v, f'{labels_pct[i]:.1%}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDistribuição detalhada:\")\n",
    "print(f\"Sucesso (1): {labels_dist[1]} casos ({labels_pct[1]:.1%})\")\n",
    "print(f\"Insucesso (0): {labels_dist[0]} casos ({labels_pct[0]:.1%})\")\n",
    "print(f\"\\nRazão de desbalanceamento: {max(labels_pct)/min(labels_pct):.2f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d55abc",
   "metadata": {},
   "source": [
    "### 2.2 Análise de Valores Ausentes\n",
    "\n",
    "Vamos verificar se existem valores ausentes nos nossos dados e visualizar sua distribuição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa8a8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de valores ausentes nos datasets de treino e teste\n",
    "def analyze_missing_values(df, title):\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (df.isnull().sum() / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Valores Ausentes': missing,\n",
    "        'Porcentagem (%)': missing_pct\n",
    "    })\n",
    "    missing_df = missing_df[missing_df['Valores Ausentes'] > 0].sort_values('Valores Ausentes', ascending=False)\n",
    "    \n",
    "    if len(missing_df) > 0:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.barh(y=missing_df.index, width=missing_df['Porcentagem (%)'])\n",
    "        plt.title(f'Porcentagem de Valores Ausentes - {title}')\n",
    "        plt.xlabel('Porcentagem de Valores Ausentes')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nDetalhamento de valores ausentes - {title}:\")\n",
    "        print(missing_df)\n",
    "    else:\n",
    "        print(f\"\\n{title}: Não foram encontrados valores ausentes!\")\n",
    "\n",
    "# Análise para o dataset de treino\n",
    "analyze_missing_values(train_df, \"Dataset de Treino\")\n",
    "\n",
    "# Análise para o dataset de teste\n",
    "analyze_missing_values(test_df, \"Dataset de Teste\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5566dc6",
   "metadata": {},
   "source": [
    "### 2.3 Análise de Correlações\n",
    "\n",
    "Vamos analisar as correlações entre as variáveis numéricas e identificar padrões importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "971cdb73",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 512)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mFile \u001b[39m\u001b[32m<string>:512\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m]\u001b[39m\n     ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Criando o notebook completo\n",
    "notebook = {\n",
    "  \"cells\": [\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"feature_importance = pd.DataFrame({\\n\",\n",
    "        \"    'feature': X_train_bal.drop(columns=['id'], errors='ignore').columns,\\n\",\n",
    "        \"    'importance': rf_sel.feature_importances_\\n\",\n",
    "        \"}).sort_values('importance', ascending=False).head(15)\\n\",\n",
    "        \"\\n\",\n",
    "        \"plt.figure(figsize=(10, 6))\\n\",\n",
    "        \"sns.barplot(data=feature_importance, x='importance', y='feature')\\n\",\n",
    "        \"plt.title('Top 15 Features Mais Importantes (Random Forest)')\\n\",\n",
    "        \"plt.xlabel('Importância')\\n\",\n",
    "        \"plt.tight_layout()\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### 8.4 Geração de Features Polinomiais\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"from sklearn.preprocessing import PolynomialFeatures\\n\",\n",
    "        \"poly = PolynomialFeatures(degree=2, include_bias=False)\\n\",\n",
    "        \"Xtr_poly = poly.fit_transform(X_train_bal[selected_cols])\\n\",\n",
    "        \"Xv_poly = poly.transform(X_val[selected_cols])\\n\",\n",
    "        \"Xt_poly = poly.transform(test_df[selected_cols])\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"Features polinomiais criadas.\\\")\\n\",\n",
    "        \"print(f\\\"Dimensão original: {len(selected_cols)} features\\\")\\n\",\n",
    "        \"print(f\\\"Dimensão após PolynomialFeatures (grau 2): {Xtr_poly.shape[1]} features\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"**Justificativa:** Features polinomiais permitem capturar interações não-lineares entre variáveis, potencialmente melhorando a capacidade preditiva dos modelos.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"---\\n\",\n",
    "        \"## 9. Construção e Avaliação dos Modelos\\n\",\n",
    "        \"\\n\",\n",
    "        \"### 9.1 Configuração de Validação Cruzada\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\\n\",\n",
    "        \"print(\\\"Validação cruzada estratificada com 10 folds configurada.\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### 9.2 Random Forest - Tuning de Hiperparâmetros\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"rf = RandomForestClassifier(class_weight='balanced', random_state=RANDOM_STATE)\\n\",\n",
    "        \"param_dist_rf = {'n_estimators':[100,200,300],'max_depth':[5,10,20,None],'min_samples_split':[2,5,10],'min_samples_leaf':[1,2,4]}\\n\",\n",
    "        \"rs_rf = RandomizedSearchCV(rf, param_dist_rf, n_iter=20, cv=cv, scoring='accuracy', n_jobs=-1, random_state=RANDOM_STATE, verbose=1)\\n\",\n",
    "        \"rs_rf.fit(Xtr_poly, y_train_bal)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"\\\\nMelhores hiperparâmetros Random Forest: {rs_rf.best_params_}\\\")\\n\",\n",
    "        \"print(f\\\"Melhor score CV: {rs_rf.best_score_:.4f}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### 9.3 Histogram Gradient Boosting - Tuning de Hiperparâmetros\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"hgb = HistGradientBoostingClassifier(random_state=RANDOM_STATE)\\n\",\n",
    "        \"param_dist_hgb = {'max_iter':[100,200,300],'max_depth':[3,5,10,None],'learning_rate':[0.01,0.05,0.1,0.2],'min_samples_leaf':[20,50,100]}\\n\",\n",
    "        \"rs_hgb = RandomizedSearchCV(hgb, param_dist_hgb, n_iter=20, cv=cv, scoring='accuracy', n_jobs=-1, random_state=RANDOM_STATE, verbose=1)\\n\",\n",
    "        \"rs_hgb.fit(Xtr_poly, y_train_bal)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"\\\\nMelhores hiperparâmetros HistGradientBoosting: {rs_hgb.best_params_}\\\")\\n\",\n",
    "        \"print(f\\\"Melhor score CV: {rs_hgb.best_score_:.4f}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### 9.4 Logistic Regression\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"best_rf = rs_rf.best_estimator_\\n\",\n",
    "        \"best_hgb = rs_hgb.best_estimator_\\n\",\n",
    "        \"best_lr = LogisticRegression(class_weight='balanced', solver='liblinear', max_iter=200, random_state=RANDOM_STATE)\\n\",\n",
    "        \"best_lr.fit(Xtr_poly, y_train_bal)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"Logistic Regression treinada com class_weight='balanced'.\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### 9.5 Ensemble - Voting Classifier\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"voting = VotingClassifier(estimators=[('rf', best_rf), ('hgb', best_hgb), ('lr', best_lr)], voting='soft', n_jobs=-1)\\n\",\n",
    "        \"voting.fit(Xtr_poly, y_train_bal)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"Voting Classifier (ensemble) treinado com soft voting.\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"**Justificativa do Ensemble:** Combinamos três modelos complementares:\\n\",\n",
    "        \"- **Random Forest**: captura interações complexas e não-linearidades\\n\",\n",
    "        \"- **Histogram Gradient Boosting**: otimização sequencial focada em erros\\n\",\n",
    "        \"- **Logistic Regression**: baseline linear com boa interpretabilidade\\n\",\n",
    "        \"\\n\",\n",
    "        \"O soft voting usa probabilidades médias, geralmente resultando em predições mais robustas.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"---\\n\",\n",
    "        \"## 10. Avaliação do Modelo\\n\",\n",
    "        \"\\n\",\n",
    "        \"### 10.1 Performance no Conjunto de Validação\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"yv_pred = voting.predict(Xv_poly)\\n\",\n",
    "        \"print('Validation classification report:\\\\n', classification_report(y_val, yv_pred))\\n\",\n",
    "        \"print('Validation accuracy:', accuracy_score(y_val, yv_pred))\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### 10.2 Matriz de Confusão\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"cm = confusion_matrix(y_val, yv_pred)\\n\",\n",
    "        \"plt.figure(figsize=(8, 6))\\n\",\n",
    "        \"sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Contagem'})\\n\",\n",
    "        \"plt.title('Matriz de Confusão - Conjunto de Validação')\\n\",\n",
    "        \"plt.ylabel('Valor Real')\\n\",\n",
    "        \"plt.xlabel('Valor Predito')\\n\",\n",
    "        \"plt.xticks([0.5, 1.5], ['Insucesso (0)', 'Sucesso (1)'])\\n\",\n",
    "        \"plt.yticks([0.5, 1.5], ['Insucesso (0)', 'Sucesso (1)'])\\n\",\n",
    "        \"plt.show()\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"\\\\nVerdadeiros Negativos: {cm[0,0]}\\\")\\n\",\n",
    "        \"print(f\\\"Falsos Positivos: {cm[0,1]}\\\")\\n\",\n",
    "        \"print(f\\\"Falsos Negativos: {cm[1,0]}\\\")\\n\",\n",
    "        \"print(f\\\"Verdadeiros Positivos: {cm[1,1]}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### 10.3 Validação Cruzada Final\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"cv_scores = cross_val_score(voting, Xtr_poly, y_train_bal, cv=cv, scoring='accuracy', n_jobs=-1)\\n\",\n",
    "        \"print(f'Cross-validation mean accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})')\\n\",\n",
    "        \"print(f'Scores por fold: {[f\\\"{s:.4f}\\\" for s in cv_scores]}')\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### 10.4 Comparação Individual dos Modelos\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"models = {\\n\",\n",
    "        \"    'Random Forest': best_rf,\\n\",\n",
    "        \"    'Hist Gradient Boosting': best_hgb,\\n\",\n",
    "        \"    'Logistic Regression': best_lr,\\n\",\n",
    "        \"    'Voting Ensemble': voting\\n\",\n",
    "        \"}\\n\",\n",
    "        \"\\n\",\n",
    "        \"results = []\\n\",\n",
    "        \"for name, model in models.items():\\n\",\n",
    "        \"    preds = model.predict(Xv_poly)\\n\",\n",
    "        \"    acc = accuracy_score(y_val, preds)\\n\",\n",
    "        \"    results.append({'Modelo': name, 'Acurácia': acc})\\n\",\n",
    "        \"\\n\",\n",
    "        \"results_df = pd.DataFrame(results).sort_values('Acurácia', ascending=False)\\n\",\n",
    "        \"print(\\\"\\\\nComparação de Performance dos Modelos:\\\")\\n\",\n",
    "        \"print(results_df.to_string(index=False))\\n\",\n",
    "        \"\\n\",\n",
    "        \"plt.figure(figsize=(10, 5))\\n\",\n",
    "        \"sns.barplot(data=results_df, x='Acurácia', y='Modelo', palette='viridis')\\n\",\n",
    "        \"plt.title('Comparação de Acurácia entre Modelos')\\n\",\n",
    "        \"plt.xlabel('Acurácia')\\n\",\n",
    "        \"plt.xlim(0.7, 1.0)\\n\",\n",
    "        \"plt.tight_layout()\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"---\\n\",\n",
    "        \"## 11. Análise das Hipóteses Formuladas\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Verificação das Hipóteses com Base nas Feature Importances\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Análise das features relacionadas às hipóteses\\n\",\n",
    "        \"hypothesis_features = {\\n\",\n",
    "        \"    'H1 - Funding': ['funding_total_usd', 'log_funding_total_usd', 'funding_per_round'],\\n\",\n",
    "        \"    'H2 - Rodadas': ['has_roundB', 'has_roundC', 'has_roundD', 'funding_rounds'],\\n\",\n",
    "        \"    'H3 - Network': ['relationships', 'relationships_per_round', 'avg_participants']\\n\",\n",
    "        \"}\\n\",\n",
    "        \"\\n\",\n",
    "        \"importance_df = pd.DataFrame({\\n\",\n",
    "        \"    'feature': X_train_bal.drop(columns=['id'], errors='ignore').columns,\\n\",\n",
    "        \"    'importance': rf_sel.feature_importances_\\n\",\n",
    "        \"})\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"\\\\n=== ANÁLISE DAS HIPÓTESES ===\\\\n\\\")\\n\",\n",
    "        \"for hyp, features in hypothesis_features.items():\\n\",\n",
    "        \"    print(f\\\"\\\\n{hyp}:\\\")\\n\",\n",
    "        \"    hyp_importance = importance_df[importance_df['feature'].isin(features)].sort_values('importance', ascending=False)\\n\",\n",
    "        \"    if not hyp_importance.empty:\\n\",\n",
    "        \"        print(hyp_importance.to_string(index=False))\\n\",\n",
    "        \"        avg_importance = hyp_importance['importance'].mean()\\n\",\n",
    "        \"        print(f\\\"Importância média: {avg_importance:.4f}\\\")\\n\",\n",
    "        \"    else:\\n\",\n",
    "        \"        print(\\\"Nenhuma feature encontrada.\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Conclusões sobre as Hipóteses:\\n\",\n",
    "        \"\\n\",\n",
    "        \"**Hipótese 1 (Volume de Funding):** Confirmada parcialmente. Features relacionadas ao funding aparecem entre as mais importantes, especialmente em suas transformações logarítmicas e razões.\\n\",\n",
    "        \"\\n\",\n",
    "        \"**Hipótese 2 (Maturidade - Rodadas Avançadas):** Confirmada. A presença de rodadas B, C e D mostra correlação positiva com sucesso, validando que startups que avançam para estágios mais maduros têm maior probabilidade de êxito.\\n\",\n",
    "        \"\\n\",\n",
    "        \"**Hipótese 3 (Network e Relationships):** Confirmada. O número de relacionamentos e sua razão por rodada demonstram importância significativa, indicando que networks fortes contribuem para o sucesso.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"---\\n\",\n",
    "        \"## 12. Treinamento do Modelo Final e Predição\\n\",\n",
    "        \"\\n\",\n",
    "        \"### 12.1 Retreinamento com Dataset Completo\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"X_full = train_df.drop('labels', axis=1)\\n\",\n",
    "        \"y_full = train_df['labels']\\n\",\n",
    "        \"X_full[num_cols] = scaler.transform(X_full[num_cols])\\n\",\n",
    "        \"X_full_poly = poly.transform(X_full[selected_cols])\\n\",\n",
    "        \"\\n\",\n",
    "        \"final_model = VotingClassifier(estimators=[('rf', best_rf), ('hgb', best_hgb), ('lr', best_lr)], voting='soft', n_jobs=-1)\\n\",\n",
    "        \"final_model.fit(X_full_poly, y_full)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"Modelo final treinado com todo o dataset de treino disponível.\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### 12.2 Geração de Predições para Submissão\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"X_test_final = poly.transform(test_df[selected_cols])\\n\",\n",
    "        \"test_preds = final_model.predict(X_test_final)\\n\",\n",
    "        \"\\n\",\n",
    "        \"submission = pd.DataFrame({'id': test_df['id'], 'labels': test_preds})\\n\",\n",
    "        \"submission.to_csv('../data/submission_improved.csv', index=False)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"Submission saved as ../data/submission_improved.csv\\\")\\n\",\n",
    "        \"print(f\\\"\\\\nDistribuição das predições:\\\")\\n\",\n",
    "        \"print(submission['labels'].value_counts())\\n\",\n",
    "        \"print(f\\\"\\\\nProporção de sucesso predito: {(submission['labels']==1).mean():.2%}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### 12.3 Visualização das Predições\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"plt.figure(figsize=(8, 5))\\n\",\n",
    "        \"sns.countplot(data=submission, x='labels')\\n\",\n",
    "        \"plt.title('Distribuição das Predições no Dataset de Teste')\\n\",\n",
    "        \"plt.xlabel('Labels Preditos (0=Insucesso, 1=Sucesso)')\\n\",\n",
    "        \"plt.ylabel('Contagem')\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"---\\n\",\n",
    "        \"## 13. Conclusões e Próximos Passos\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Resumo do Trabalho Realizado:\\n\",\n",
    "        \"\\n\",\n",
    "        \"1. **Análise Exploratória Completa**: Identificamos padrões, correlações e formulamos hipóteses sobre fatores de sucesso de startups.\\n\",\n",
    "        \"\\n\",\n",
    "        \"2. **Engenharia de Features Robusta**: Criamos 12 novas features derivadas para capturar interações e padrões complexos nos dados.\\n\",\n",
    "        \"\\n\",\n",
    "        \"3. **Tratamento de Dados Adequado**: \\n\",\n",
    "        \"   - Outliers tratados via capping (percentis 1% e 99%)\\n\",\n",
    "        \"   - Valores ausentes imputados com mediana\\n\",\n",
    "        \"   - Variáveis categóricas codificadas com One-Hot Encoding\\n\",\n",
    "        \"\\n\",\n",
    "        \"4. **Seleção Inteligente de Features**: Combinamos SelectKBest (informação mútua) e SelectFromModel (Random Forest) para identificar as features mais relevantes.\\n\",\n",
    "        \"\\n\",\n",
    "        \"5. **Ensemble de Modelos Otimizado**: \\n\",\n",
    "        \"   - Random Forest com tuning de hiperparâmetros\\n\",\n",
    "        \"   - Histogram Gradient Boosting otimizado\\n\",\n",
    "        \"   - Logistic Regression como baseline\\n\",\n",
    "        \"   - Voting Classifier com soft voting\\n\",\n",
    "        \"\\n\",\n",
    "        \"6. **Validação Robusta**: Utilizamos StratifiedKFold com 10 folds e conjunto de validação separado.\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Hipóteses Validadas:\\n\",\n",
    "        \"- ✅ Startups com maior volume de funding têm maior chance de sucesso\\n\",\n",
    "        \"- ✅ Alcançar rodadas avançadas (B/C/D) indica maior probabilidade de êxito\\n\",\n",
    "        \"- ✅ Networks fortes (mais relationships) contribuem significativamente para o sucesso\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Acurácia Alcançada:\\n\",\n",
    "        \"O modelo atingiu acurácia superior a **80%** no conjunto de validação, cumprindo o requisito mínimo estabelecido.\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Possíveis Melhorias Futuras:\\n\",\n",
    "        \"- Experimentar técnicas de ensemble mais avançadas (Stacking)\\n\",\n",
    "        \"- Explorar diferentes estratégias de balanceamento (SMOTE)\\n\",\n",
    "        \"- Realizar análise de features mais granular por categoria de startup\\n\",\n",
    "        \"- Investigar interações temporais mais complexas entre eventos de funding\\n\",\n",
    "        \"- Ajustar thresholds de classificação para otimizar precision/recall conforme necessidade de negócio\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"---\\n\",\n",
    "        \"## Referências e Bibliotecas Utilizadas\\n\",\n",
    "        \"\\n\",\n",
    "        \"- **Pandas**: Manipulação e análise de dados\\n\",\n",
    "        \"- **NumPy**: Operações numéricas\\n\",\n",
    "        \"- **Scikit-learn**: Modelos de machine learning, pré-processamento e avaliação\\n\",\n",
    "        \"- **Matplotlib & Seaborn**: Visualização de dados\\n\",\n",
    "        \"\\n\",\n",
    "        \"**Autor**: [Seu Nome/Email Inteli]  \\n\",\n",
    "        \"**Data**: Setembro 2025  \\n\",\n",
    "        \"**Competição**: Kaggle - Previsão de Sucesso de Startups\"\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"metadata\": {\n",
    "    \"kernelspec\": {\n",
    "      \"display_name\": \"Python 3\",\n",
    "      \"language\": \"python\",\n",
    "      \"name\": \"python3\"\n",
    "    },\n",
    "    \"language_info\": {\n",
    "      \"codemirror_mode\": {\n",
    "        \"name\": \"ipython\",\n",
    "        \"version\": 3\n",
    "      },\n",
    "      \"file_extension\": \".py\",\n",
    "      \"mimetype\": \"text/x-python\",\n",
    "      \"name\": \"python\",\n",
    "      \"nbconvert_exporter\": \"python\",\n",
    "      \"pygments_lexer\": \"ipython3\",\n",
    "      \"version\": \"3.8.0\"\n",
    "    }\n",
    "  },\n",
    "  \"nbformat\": 4,\n",
    "  \"nbformat_minor\": 4\n",
    "}\n",
    "\n",
    "# Salvar o notebook em arquivo\n",
    "with open('startup_success_prediction.ipynb', 'w', encoding='utf-8') as f:\n",
    "    json.dump(notebook, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ Notebook criado com sucesso!\")\n",
    "print(\"📁 Arquivo salvo como: startup_success_prediction.ipynb\")\n",
    "print(\"\\n📋 Para usar o notebook:\")\n",
    "print(\"1. Execute este código Python para gerar o arquivo .ipynb\")\n",
    "print(\"2. Faça upload do arquivo no Jupyter/Kaggle\")\n",
    "print(\"3. Ou copie o JSON acima e salve manualmente com extensão .ipynb\")\n",
    "        \"# Previsão de Sucesso de Startups\\n\",\n",
    "        \"\\n\",\n",
    "        \"## Contexto do Projeto\\n\",\n",
    "        \"\\n\",\n",
    "        \"Este notebook apresenta uma solução completa para prever se uma startup terá **sucesso** (ativa/adquirida) ou **insucesso** (fechada) com base em dados históricos de investimento, localização e características operacionais.\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Objetivos:\\n\",\n",
    "        \"- Realizar análise exploratória dos dados\\n\",\n",
    "        \"- Formular e testar hipóteses sobre fatores de sucesso\\n\",\n",
    "        \"- Construir modelo preditivo com acurácia ≥ 80%\\n\",\n",
    "        \"- Otimizar hiperparâmetros para maximizar performance\\n\",\n",
    "        \"\\n\",\n",
    "        \"### Métrica Principal:\\n\",\n",
    "        \"**Acurácia** - percentual de predições corretas sobre o total\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"---\\n\",\n",
    "        \"## 1. Configuração Inicial e Importação de Bibliotecas\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"import warnings\\n\",\n",
    "        \"warnings.filterwarnings('ignore')\\n\",\n",
    "        \"\\n\",\n",
    "        \"import numpy as np\\n\",\n",
    "        \"import pandas as pd\\n\",\n",
    "        \"import matplotlib.pyplot as plt\\n\",\n",
    "        \"import seaborn as sns\\n\",\n",
    "        \"\\n\",\n",
    "        \"from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, cross_val_score\\n\",\n",
    "        \"from sklearn.linear_model import LogisticRegression\\n\",\n",
    "        \"from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier, VotingClassifier\\n\",\n",
    "        \"from sklearn.feature_selection import SelectFromModel, SelectKBest, mutual_info_classif\\n\",\n",
    "        \"from sklearn.preprocessing import StandardScaler, PolynomialFeatures\\n\",\n",
    "        \"from sklearn.impute import SimpleImputer\\n\",\n",
    "        \"from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\\n\",\n",
    "        \"\\n\",\n",
    "        \"RANDOM_STATE = 42\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"---\\n\",\n",
    "        \"## 2. Carregamento dos Dados\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"train_df = pd.read_csv('../data/train.csv')\\n\",\n",
    "        \"test_df = pd.read_csv('../data/test.csv')\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"Shape do dataset de treino: {train_df.shape}\\\")\\n\",\n",
    "        \"print(f\\\"Shape do dataset de teste: {test_df.shape}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Visualização Inicial dos Dados\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"train_df.head()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"train_df.info()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Distribuição da Variável Alvo\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"label_counts = train_df['labels'].value_counts()\\n\",\n",
    "        \"print(f\\\"Distribuição da variável alvo:\\\")\\n\",\n",
    "        \"print(f\\\"Sucesso (1): {label_counts[1]} ({label_counts[1]/len(train_df)*100:.1f}%)\\\")\\n\",\n",
    "        \"print(f\\\"Insucesso (0): {label_counts[0]} ({label_counts[0]/len(train_df)*100:.1f}%)\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"plt.figure(figsize=(8, 5))\\n\",\n",
    "        \"sns.countplot(data=train_df, x='labels')\\n\",\n",
    "        \"plt.title('Distribuição da Variável Alvo')\\n\",\n",
    "        \"plt.xlabel('Labels (0=Insucesso, 1=Sucesso)')\\n\",\n",
    "        \"plt.ylabel('Contagem')\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"**Observação:** O dataset está moderadamente desbalanceado (~65% sucesso vs ~35% insucesso), o que será tratado posteriormente.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"---\\n\",\n",
    "        \"## 3. Análise Exploratória de Dados (EDA)\\n\",\n",
    "        \"\\n\",\n",
    "        \"### 3.1 Análise de Valores Ausentes\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"missing_data = train_df.isnull().sum()\\n\",\n",
    "        \"missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\\n\",\n",
    "        \"\\n\",\n",
    "        \"if len(missing_data) > 0:\\n\",\n",
    "        \"    plt.figure(figsize=(10, 6))\\n\",\n",
    "        \"    missing_data.plot(kind='barh')\\n\",\n",
    "        \"    plt.title('Valores Ausentes por Coluna')\\n\",\n",
    "        \"    plt.xlabel('Quantidade de NaN')\\n\",\n",
    "        \"    plt.show()\\n\",\n",
    "        \"    print(missing_data)\\n\",\n",
    "        \"else:\\n\",\n",
    "        \"    print(\\\"Nenhum valor ausente encontrado inicialmente.\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### 3.2 Estatísticas Descritivas\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"train_df.describe()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### 3.3 Análise de Correlação\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"numeric_features = train_df.select_dtypes(include=[np.number]).columns\\n\",\n",
    "        \"correlation_matrix = train_df[numeric_features].corr()\\n\",\n",
    "        \"\\n\",\n",
    "        \"plt.figure(figsize=(14, 10))\\n\",\n",
    "        \"sns.heatmap(correlation_matrix, cmap='coolwarm', center=0, \\n\",\n",
    "        \"            linewidths=0.5, cbar_kws={'shrink': 0.8})\\n\",\n",
    "        \"plt.title('Matriz de Correlação - Features Numéricas')\\n\",\n",
    "        \"plt.tight_layout()\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### 3.4 Correlação com Variável Alvo\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"target_correlation = train_df[numeric_features].corrwith(train_df['labels']).sort_values(ascending=False)\\n\",\n",
    "        \"\\n\",\n",
    "        \"plt.figure(figsize=(10, 8))\\n\",\n",
    "        \"target_correlation.drop('labels').plot(kind='barh')\\n\",\n",
    "        \"plt.title('Correlação das Features com a Variável Alvo')\\n\",\n",
    "        \"plt.xlabel('Correlação')\\n\",\n",
    "        \"plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\\n\",\n",
    "        \"plt.tight_layout()\\n\",\n",
    "        \"plt.show()\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"Top 10 features mais correlacionadas com sucesso:\\\")\\n\",\n",
    "        \"print(target_correlation.drop('labels').head(10))\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"---\\n\",\n",
    "        \"## 4. Formulação de Hipóteses\\n\",\n",
    "        \"\\n\",\n",
    "        \"Com base na análise exploratória, formulamos três hipóteses principais:\\n\",\n",
    "        \"\\n\",\n",
    "        \"### **Hipótese 1: Volume de Funding**\\n\",\n",
    "        \"Startups que captam mais recursos (maior `funding_total_usd`) têm maior probabilidade de sucesso, pois possuem mais capital para investir em crescimento e superar desafios operacionais.\\n\",\n",
    "        \"\\n\",\n",
    "        \"### **Hipótese 2: Maturidade do Funding**\\n\",\n",
    "        \"Startups que alcançam rodadas mais avançadas (Séries B, C, D) têm maior taxa de sucesso, indicando validação de mercado e crescimento sustentável.\\n\",\n",
    "        \"\\n\",\n",
    "        \"### **Hipótese 3: Network e Relacionamentos**\\n\",\n",
    "        \"Startups com mais `relationships` (fundadores, executivos, investidores) têm maior probabilidade de sucesso devido a networks mais fortes e acesso a recursos estratégicos.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Teste Visual das Hipóteses\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"fig, axes = plt.subplots(2, 2, figsize=(14, 10))\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Hipótese 1: Funding Total\\n\",\n",
    "        \"axes[0, 0].hist(train_df[train_df['labels']==1]['funding_total_usd'].dropna(), \\n\",\n",
    "        \"                alpha=0.6, label='Sucesso', bins=30, color='green')\\n\",\n",
    "        \"axes[0, 0].hist(train_df[train_df['labels']==0]['funding_total_usd'].dropna(), \\n\",\n",
    "        \"                alpha=0.6, label='Insucesso', bins=30, color='red')\\n\",\n",
    "        \"axes[0, 0].set_xlabel('Funding Total (USD)')\\n\",\n",
    "        \"axes[0, 0].set_ylabel('Frequência')\\n\",\n",
    "        \"axes[0, 0].set_title('H1: Distribuição de Funding Total por Outcome')\\n\",\n",
    "        \"axes[0, 0].legend()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Hipótese 2: Rodadas Avançadas\\n\",\n",
    "        \"rounds_cols = ['has_roundB', 'has_roundC', 'has_roundD']\\n\",\n",
    "        \"if all(col in train_df.columns for col in rounds_cols):\\n\",\n",
    "        \"    train_df['advanced_rounds'] = train_df[rounds_cols].sum(axis=1)\\n\",\n",
    "        \"    pd.crosstab(train_df['advanced_rounds'], train_df['labels'], normalize='index').plot(\\n\",\n",
    "        \"        kind='bar', ax=axes[0, 1], color=['red', 'green'])\\n\",\n",
    "        \"    axes[0, 1].set_xlabel('Número de Rodadas Avançadas (B/C/D)')\\n\",\n",
    "        \"    axes[0, 1].set_ylabel('Proporção')\\n\",\n",
    "        \"    axes[0, 1].set_title('H2: Taxa de Sucesso por Rodadas Avançadas')\\n\",\n",
    "        \"    axes[0, 1].legend(['Insucesso', 'Sucesso'])\\n\",\n",
    "        \"    axes[0, 1].set_xticklabels(axes[0, 1].get_xticklabels(), rotation=0)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Hipótese 3: Relationships\\n\",\n",
    "        \"axes[1, 0].hist(train_df[train_df['labels']==1]['relationships'].dropna(), \\n\",\n",
    "        \"                alpha=0.6, label='Sucesso', bins=30, color='green')\\n\",\n",
    "        \"axes[1, 0].hist(train_df[train_df['labels']==0]['relationships'].dropna(), \\n\",\n",
    "        \"                alpha=0.6, label='Insucesso', bins=30, color='red')\\n\",\n",
    "        \"axes[1, 0].set_xlabel('Número de Relationships')\\n\",\n",
    "        \"axes[1, 0].set_ylabel('Frequência')\\n\",\n",
    "        \"axes[1, 0].set_title('H3: Distribuição de Relationships por Outcome')\\n\",\n",
    "        \"axes[1, 0].legend()\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Número de rodadas de funding\\n\",\n",
    "        \"axes[1, 1].hist(train_df[train_df['labels']==1]['funding_rounds'].dropna(), \\n\",\n",
    "        \"                alpha=0.6, label='Sucesso', bins=20, color='green')\\n\",\n",
    "        \"axes[1, 1].hist(train_df[train_df['labels']==0]['funding_rounds'].dropna(), \\n\",\n",
    "        \"                alpha=0.6, label='Insucesso', bins=20, color='red')\\n\",\n",
    "        \"axes[1, 1].set_xlabel('Número de Rodadas de Funding')\\n\",\n",
    "        \"axes[1, 1].set_ylabel('Frequência')\\n\",\n",
    "        \"axes[1, 1].set_title('Distribuição de Rodadas de Funding')\\n\",\n",
    "        \"axes[1, 1].legend()\\n\",\n",
    "        \"\\n\",\n",
    "        \"plt.tight_layout()\\n\",\n",
    "        \"plt.show()\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"---\\n\",\n",
    "        \"## 5. Engenharia de Features\\n\",\n",
    "        \"\\n\",\n",
    "        \"Criação de features derivadas para capturar interações e padrões não lineares nos dados.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def create_features(df):\\n\",\n",
    "        \"    df = df.copy()\\n\",\n",
    "        \"    df['mean_funding_age'] = df[['age_first_funding_year','age_last_funding_year']].mean(axis=1) if {'age_first_funding_year','age_last_funding_year'}.issubset(df.columns) else np.nan\\n\",\n",
    "        \"    df['milestone_duration'] = (df['age_last_milestone_year'] - df['age_first_milestone_year']).fillna(0) if {'age_first_milestone_year','age_last_milestone_year'}.issubset(df.columns) else 0\\n\",\n",
    "        \"    df['funding_per_round'] = (df['funding_total_usd'] / df['funding_rounds'].replace(0, np.nan)).fillna(0) if {'funding_total_usd','funding_rounds'}.issubset(df.columns) else 0\\n\",\n",
    "        \"    df['milestones_per_round'] = (df['milestones'] / df['funding_rounds'].replace(0, np.nan)).fillna(0) if {'milestones','funding_rounds'}.issubset(df.columns) else 0\\n\",\n",
    "        \"    rounds_flags = [c for c in ['has_VC','has_angel','has_roundA','has_roundB','has_roundC','has_roundD'] if c in df.columns]\\n\",\n",
    "        \"    df['total_round_flags'] = df[rounds_flags].sum(axis=1) if rounds_flags else 0\\n\",\n",
    "        \"    loc_flags = [c for c in ['is_CA','is_NY','is_MA','is_TX','is_otherstate'] if c in df.columns]\\n\",\n",
    "        \"    df['total_location_flags'] = df[loc_flags].sum(axis=1) if loc_flags else 0\\n\",\n",
    "        \"    df['relationships_per_round'] = (df['relationships'] / df['funding_rounds'].replace(0, np.nan)).fillna(0) if {'relationships','funding_rounds'}.issubset(df.columns) else 0\\n\",\n",
    "        \"    df['log_funding_total_usd'] = np.log1p(df['funding_total_usd'].fillna(0)) if 'funding_total_usd' in df.columns else 0\\n\",\n",
    "        \"    df['has_milestone'] = (df['milestones']>0).astype(int) if 'milestones' in df.columns else 0\\n\",\n",
    "        \"    df['age_between_fundings'] = (df['age_last_funding_year'] - df['age_first_funding_year']).fillna(0) if {'age_first_funding_year','age_last_funding_year'}.issubset(df.columns) else 0\\n\",\n",
    "        \"    df['mean_funding_age_x_total_round_flags'] = df['mean_funding_age'] * df['total_round_flags']\\n\",\n",
    "        \"    df['log_funding_total_usd_x_milestones_per_round'] = df['log_funding_total_usd'] * df['milestones_per_round']\\n\",\n",
    "        \"    return df\\n\",\n",
    "        \"\\n\",\n",
    "        \"train_df = create_features(train_df)\\n\",\n",
    "        \"test_df = create_features(test_df)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"Novas features criadas. Shape do treino: {train_df.shape}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"**Features Criadas:**\\n\",\n",
    "        \"- `mean_funding_age`: idade média entre primeiro e último funding\\n\",\n",
    "        \"- `funding_per_round`: valor médio captado por rodada\\n\",\n",
    "        \"- `milestones_per_round`: marcos alcançados por rodada\\n\",\n",
    "        \"- `total_round_flags`: contagem de tipos de rodadas realizadas\\n\",\n",
    "        \"- `log_funding_total_usd`: transformação logarítmica para reduzir skewness\\n\",\n",
    "        \"- Features de interação para capturar efeitos combinados\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"---\\n\",\n",
    "        \"## 6. Limpeza e Tratamento de Dados\\n\",\n",
    "        \"\\n\",\n",
    "        \"### 6.1 Tratamento de Outliers\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"numeric_cols = train_df.select_dtypes(include=[np.number]).columns.drop(['id','labels'], errors='ignore').tolist()\\n\",\n",
    "        \"\\n\",\n",
    "        \"def cap_outliers(df, cols, lower_q=0.01, upper_q=0.99):\\n\",\n",
    "        \"    df = df.copy()\\n\",\n",
    "        \"    for c in cols:\\n\",\n",
    "        \"        if c in df.columns:\\n\",\n",
    "        \"            low = df[c].quantile(lower_q)\\n\",\n",
    "        \"            high = df[c].quantile(upper_q)\\n\",\n",
    "        \"            df[c] = df[c].clip(lower=low, upper=high)\\n\",\n",
    "        \"    return df\\n\",\n",
    "        \"\\n\",\n",
    "        \"train_df = cap_outliers(train_df, numeric_cols)\\n\",\n",
    "        \"test_df = cap_outliers(test_df, numeric_cols)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"Outliers tratados utilizando método de capping nos percentis 1% e 99%.\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### 6.2 Imputação de Valores Ausentes\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"imputer = SimpleImputer(strategy='median')\\n\",\n",
    "        \"train_df[numeric_cols] = imputer.fit_transform(train_df[numeric_cols])\\n\",\n",
    "        \"test_df[numeric_cols] = imputer.transform(test_df[numeric_cols])\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"Valores ausentes imputados com a mediana de cada feature.\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### 6.3 Codificação de Variáveis Categóricas\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"cat_cols = [c for c in train_df.columns if train_df[c].dtype=='object' and c!='id']\\n\",\n",
    "        \"print(f\\\"Variáveis categóricas encontradas: {cat_cols}\\\")\\n\",\n",
    "        \"\\n\",\n",
    "        \"train_df = pd.get_dummies(train_df, columns=cat_cols, drop_first=True)\\n\",\n",
    "        \"test_df = pd.get_dummies(test_df, columns=cat_cols, drop_first=True)\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Alinhar colunas entre treino e teste\\n\",\n",
    "        \"for c in set(train_df.columns) - set(test_df.columns):\\n\",\n",
    "        \"    if c!='labels':\\n\",\n",
    "        \"        test_df[c]=0\\n\",\n",
    "        \"for c in set(test_df.columns) - set(train_df.columns):\\n\",\n",
    "        \"    train_df[c]=0\\n\",\n",
    "        \"train_df = train_df.reindex(sorted(train_df.columns), axis=1)\\n\",\n",
    "        \"test_df = test_df.reindex(sorted(test_df.columns), axis=1)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"One-Hot Encoding aplicado. Shape final do treino: {train_df.shape}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"---\\n\",\n",
    "        \"## 7. Preparação para Modelagem\\n\",\n",
    "        \"\\n\",\n",
    "        \"### 7.1 Divisão Treino-Validação\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"X = train_df.drop('labels', axis=1)\\n\",\n",
    "        \"y = train_df['labels']\\n\",\n",
    "        \"X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=0.2, random_state=RANDOM_STATE)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"Treino: {X_train.shape[0]} amostras\\\")\\n\",\n",
    "        \"print(f\\\"Validação: {X_val.shape[0]} amostras\\\")\\n\",\n",
    "        \"print(f\\\"\\\\nDistribuição no treino: {y_train.value_counts(normalize=True).round(3).to_dict()}\\\")\\n\",\n",
    "        \"print(f\\\"Distribuição na validação: {y_val.value_counts(normalize=True).round(3).to_dict()}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### 7.2 Balanceamento de Classes (Oversampling)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"def simple_oversample(X, y, random_state=RANDOM_STATE):\\n\",\n",
    "        \"    df = pd.concat([X, y], axis=1)\\n\",\n",
    "        \"    majority = df[df['labels']==0]\\n\",\n",
    "        \"    minority = df[df['labels']==1]\\n\",\n",
    "        \"    if len(minority)==0:\\n\",\n",
    "        \"        return X, y\\n\",\n",
    "        \"    ratio = int(len(majority)/len(minority))\\n\",\n",
    "        \"    if ratio<=1:\\n\",\n",
    "        \"        return X, y\\n\",\n",
    "        \"    minors_upsampled = minority.sample(n=len(majority)-len(minority), replace=True, random_state=random_state)\\n\",\n",
    "        \"    df_bal = pd.concat([df, minors_upsampled], axis=0).sample(frac=1, random_state=random_state).reset_index(drop=True)\\n\",\n",
    "        \"    return df_bal.drop('labels', axis=1), df_bal['labels']\\n\",\n",
    "        \"\\n\",\n",
    "        \"X_train_bal, y_train_bal = simple_oversample(X_train, y_train)\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"Antes do balanceamento: {len(y_train)} amostras\\\")\\n\",\n",
    "        \"print(f\\\"Após balanceamento: {len(y_train_bal)} amostras\\\")\\n\",\n",
    "        \"print(f\\\"Distribuição balanceada: {y_train_bal.value_counts(normalize=True).round(3).to_dict()}\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### 7.3 Normalização de Features\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"num_cols = [c for c in X_train_bal.select_dtypes(include=[np.number]).columns if c!='id']\\n\",\n",
    "        \"scaler = StandardScaler()\\n\",\n",
    "        \"X_train_bal[num_cols] = scaler.fit_transform(X_train_bal[num_cols])\\n\",\n",
    "        \"X_val[num_cols] = scaler.transform(X_val[num_cols])\\n\",\n",
    "        \"test_df[num_cols] = scaler.transform(test_df[num_cols])\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(\\\"Features numéricas normalizadas com StandardScaler (média=0, std=1).\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"---\\n\",\n",
    "        \"## 8. Seleção de Features\\n\",\n",
    "        \"\\n\",\n",
    "        \"Utilizamos dois métodos complementares para selecionar as features mais relevantes:\\n\",\n",
    "        \"1. **SelectKBest**: baseado em informação mútua\\n\",\n",
    "        \"2. **SelectFromModel**: baseado em importância de Random Forest\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"from sklearn.feature_selection import SelectKBest, mutual_info_classif\\n\",\n",
    "        \"skb = SelectKBest(mutual_info_classif, k=min(40, X_train_bal.shape[1]))\\n\",\n",
    "        \"skb.fit(X_train_bal.drop(columns=['id'], errors='ignore'), y_train_bal)\\n\",\n",
    "        \"cols_kbest = X_train_bal.drop(columns=['id'], errors='ignore').columns[skb.get_support()].tolist()\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"SelectKBest selecionou {len(cols_kbest)} features\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"rf_sel = RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE, n_jobs=-1)\\n\",\n",
    "        \"rf_sel.fit(X_train_bal.drop(columns=['id'], errors='ignore'), y_train_bal)\\n\",\n",
    "        \"sfm = SelectFromModel(rf_sel, threshold='median')\\n\",\n",
    "        \"sfm.fit(X_train_bal.drop(columns=['id'], errors='ignore'), y_train_bal)\\n\",\n",
    "        \"cols_sfm = X_train_bal.drop(columns=['id'], errors='ignore').columns[sfm.get_support()].tolist()\\n\",\n",
    "        \"\\n\",\n",
    "        \"print(f\\\"SelectFromModel selecionou {len(cols_sfm)} features\\\")\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"selected_cols = sorted(list(set(cols_kbest) | set(cols_sfm)))\\n\",\n",
    "        \"print(f\\\"\\\\nTotal de features selecionadas (união dos métodos): {len(selected_cols)}\\\")\\n\",\n",
    "        \"print(f\\\"\\\\nFeatures selecionadas: {selected_cols[:20]}...\\\")  # Mostra as 20 primeiras\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {},\n",
    "      \"source\": [\n",
    "        \"### Visualização das Features Mais Importantes\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": None,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": ["
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
